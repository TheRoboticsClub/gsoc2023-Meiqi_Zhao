<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-09T14:37:23-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/feed.xml</id><title type="html">Meiqi Zhao | JdeRobot x GSoC2023</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">GSoC 2023: Project Recap</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week18/" rel="alternate" type="text/html" title="GSoC 2023: Project Recap" /><published>2023-10-08T04:00:00-04:00</published><updated>2023-10-08T04:00:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week18</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week18/"><![CDATA[<h2 id="summary">Summary</h2>
<p>The past 18 weeks have been an incredible journey filled with learning, challenges, and achievements. In the initial phase, we successfully trained an autonomous driving agent that exhibited obstacle avoidance skills and could follow a predefined route while responding to high-level turning commands. This agent was built using imitation learning and deep neural networks. During the latter part of the program, our focus shifted to expanding the capabilities of Behavior Metrics. We introduced support for generating traffic, allowing users to configure scenarios with other vehicles and pedestrians. Additionally, we added a new follow-route task, enabling agents to follow predefined routes involving various turns and junctions. To accurately evaluate the performance of agents in these new scenarios, we designed and implemented new evaluation metrics. These metrics included route completion ratios, success rates, weighted success rates, and detailed infraction tracking, providing a comprehensive assessment of agent performance.</p>

<p>None of this would have been possible without the guidance and support of my mentors, Sergio Paniego Blanco and Nikhil Paliwal, whose expertise and dedication were invaluable throughout this journey. I’d also like to express my gratitude to Jose Maria Canas for his insightful contributions.Thank you to Google for organizing GSoC, and to everyone who has been a part of this journey!</p>

<p>Below is a brief overview of the work completed.</p>

<h2 id="demo">Demo</h2>
<center>
<iframe width="840" height="472" src="https://www.youtube.com/embed/yfScvcrjYkg?si=o6Yd0l3BmUdnHYl1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</center>

<h2 id="code">Code</h2>

<h3 id="codebases">Codebases</h3>
<ul>
  <li><a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao">Segmentation-based End-to-End Imitation Learning for Autonomous Driving</a></li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics">Behavior Metrics</a></li>
</ul>

<h3 id="issues">Issues</h3>
<ul>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/issues/603">#603</a> Add a launch for Town 01 with parked vehicles</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/issues/613">#613</a> Town01 scenario with dynamic objects</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/issues/625">#625</a> Generate traffic using CARLA Python API</li>
</ul>

<h3 id="pull-requests">Pull Requests</h3>
<ul>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/606">#606</a> new launch file for Town01 with a stopped car</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/620">#620</a> added on npc vehicle with ad agent</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/626">#626</a> generate traffic</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/627">#627</a> Update instructions on installing carla-birdeye-view</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/632">#632</a> Added segmentation based imitation learning model as a new brain</li>
  <li><a href="https://github.com/JdeRobot/BehaviorMetrics/pull/643">#643</a> Add support for a new follow-route task and evaluation metrics</li>
</ul>

<h2 id="weekly-progress">Weekly Progress</h2>
<h3 id="week-17"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week17">Week 17</a></h3>
<p>In Week 17, the main focus was on expanding the evaluation metrics in Behavior Metrics to accommodate the new “follow-route” task.</p>

<h3 id="week-16"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week16">Week 16</a></h3>
<p>In Week 16, the primary focus was on developing Behavior Metrics. Specifically, the team worked on implementing the new “follow-route” task in Behavior Metrics.</p>

<h3 id="week-15"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week15">Week 15</a></h3>
<p>During Week 15, the focus was on improving model performance by experimenting with different architectures. Simultaneously, work continued on expanding Behavior Metrics by introducing user customization for different task types and integrating additional evaluation metrics.</p>

<h3 id="week-14"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week14">Week 14</a></h3>
<p>During Week 14, the primary focus was on integrating the current model into Behavior Metrics to complete the project pipeline. The model was successfully added as a new brain in Behavior Metrics, enabling its use within the platform.</p>

<h3 id="week-12--13"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week12">Week 12 &amp; 13</a></h3>
<p>During Weeks 12 and 13, there was a focus on improving the model’s obstacle avoidance capabilities.</p>

<h3 id="week-11"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week11">Week 11</a></h3>
<p>In Week 11, efforts to enhance the model’s performance, particularly in obstacle avoidance, continued. Additionally, progress was made in integrating traffic generation functionality into the Behavior Metrics platform.</p>

<h3 id="week-10"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week10">Week 10</a></h3>
<p>In Week 10, the focus shifted towards refining and evaluating the model’s performance. Efforts also began to integrate the model into the Behavior Metrics platform, expanding its capabilities to include traffic.</p>

<h3 id="week-9"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week9">Week 9</a></h3>
<p>In Week 9, we implemented Data Aggregation (DAgger) to iteratively enhance the model’s behavior and refined the evaluation metrics to detect whether the model correctly follows turning instructions.</p>

<h3 id="week-8"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">Week 8</a></h3>
<p>In Week 8, we continued to address the “halting” problem encountered in the model, particularly when making turns at intersections. The team explored various strategies to optimize data collection, including data trimming and prioritization.</p>

<h3 id="week-7"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week7">Week 7</a></h3>
<p>In Week 7, the focus was on enhancing the model’s performance, particularly addressing the “halting” problem where the agent occasionally stops and gets stuck during navigation.</p>

<h3 id="week-6"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week6">Week 6</a></h3>
<p>In Week 6, the main focus was on enhancing the evaluation process by incorporating more sophisticated metrics inspired by the CARLA Leaderboard.</p>

<h3 id="week-5"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week5">Week 5</a></h3>
<p>In Week 5, the primary focus was on refining the model’s adherence to traffic lights by incorporating traffic light status as an additional input and experimenting with one-hot encoding for high-level commands.</p>

<h3 id="week-4"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week4">Week 4</a></h3>
<p>This week, efforts were focused on enhancing the model’s ability to make turns at intersections in any direction by incorporating high-level commands. Data collection was adjusted to record these commands, and the model architecture was updated to accommodate them.</p>

<h3 id="week-3"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week3">Week 3</a></h3>
<p>This week, the focus was on improving the versatility of the lane-following model by enabling it to navigate routes with turns and intersections.</p>

<h3 id="week-2"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week2">Week 2</a></h3>
<p>In the second week of coding, the focus was on improving the training data quality. The primary enhancement was the introduction of noise injection into the expert agent’s control commands to simulate recovery from disturbances. This approach proved effective in teaching the model how to auto-correct when it deviates from the center of the lane.</p>

<h3 id="week-1"><a href="/gsoc2023-Meiqi_Zhao/blog/2023/week1">Week 1</a></h3>
<p>Implemented a data collection tool and collecting sample data, primarily consisting of simple scenarios with straight routes and dynamic obstacles. Simultaneously, a modified model, DeepestLSTMTinyPilotNet, was explored and trained on the collected data.</p>

<h3 id="community-bonding-week-2"><a href="http://127.0.0.1:4000/gsoc2023-Meiqi_Zhao/blog/2023/community-bonding-week-2/">Community Bonding Week 2</a></h3>
<p>During the second week of community bonding, the focus was on researching data collection methods for the project.</p>

<h3 id="community-bonding-week-1"><a href="http://127.0.0.1:4000/gsoc2023-Meiqi_Zhao/blog/2023/community-bonding-week-1/">Community Bonding Week 1</a></h3>
<p>The week involved setting up the blog website, conducting literature research, and laying the project’s groundwork.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="FinalEvaluation" /><category term="GSoC" /><summary type="html"><![CDATA[A recap of the Summer of Code journey]]></summary></entry><entry><title type="html">Week 17: Sep 18 ~ Sep 24</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week17/" rel="alternate" type="text/html" title="Week 17: Sep 18 ~ Sep 24" /><published>2023-09-25T04:00:00-04:00</published><updated>2023-09-25T04:00:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week17</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week17/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>In the previous week, we successfully introduced support for a new follow-route task in Behavior Metrics. This task requires the agent to follow a sequence of high-level commands, making turns at junctions to reach a destination. However, with this new task comes the need for additional evaluation metrics tailored to its unique requirements. We have decided to incorporate the same evaluation metrics as those <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">previously used during the development of our follow-route agent</a>. These metrics include a route completion ratio, success rate, weighted success rate, counts of various infractions, and an overall driving score that considers route completion and infractions. The challenge now is to integrate these evaluation metrics into Behavior Metrics’ existing workflow.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Update <a href="https://github.com/JdeRobot/BehaviorMetrics/pull/632">PR</a></li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Implement new evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Create a pull request for the new evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Produce the final demo video</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Upload models and dataset to Hugging Face</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="workflow">Workflow</h3>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/behavior_metrics_framework.png" width="800" />
  </center>
</figure>
<p>The image above outlines the current workflow of Behavior Metrics, particularly in the context of the newly added follow-route task:</p>
<ol>
  <li>The main program, <code class="language-plaintext highlighter-rouge">driver_carla.py</code>, takes a YAML file containing configurations for the robot and experiments, a <code class="language-plaintext highlighter-rouge">.launch</code> file that initiates the CARLA ROS Bridge, and a test suite file containing all the testing routes.</li>
  <li>Within the YAML file, multiple models and worlds can be listed for evaluation. For each combination of (world, model, route), the Test Suite Manager is launched to execute a single experiment and record the evaluation metrics.</li>
  <li>Evaluation metrics are obtained from two sources: (1) data such as odometry and collision information published in the ROS topics by the CARLA ROS Bridge and (2) additional infractions calculated and recorded by the brain, including traffic light infractions and termination causes.</li>
  <li>These two sources of evaluation metrics are processed and merged to generate the final performance evaluation report.</li>
</ol>

<h3 id="issues">Issues</h3>
<p>Several issues need attention in the future:</p>
<ul>
  <li>Test Suite Manager almost never exits with code 0, causing repeated attempts.</li>
  <li>A delay exists between stopping the recording of metrics and stopping the simulation.</li>
  <li>The newly added evaluation metrics are not plotted in the output image.</li>
</ul>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="BehaviorMetrics" /><category term="EvaluationMetrics" /><summary type="html"><![CDATA[Expand evaluation metrics in Behavior Metrics]]></summary></entry><entry><title type="html">Week 16: Sep 11 ~ Sep 17</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week16/" rel="alternate" type="text/html" title="Week 16: Sep 11 ~ Sep 17" /><published>2023-09-19T04:30:00-04:00</published><updated>2023-09-19T04:30:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week16</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week16/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>Starting this week, our primary focus shifts towards the development of Behavior Metrics. Initially, Behavior Metrics only supported the follow-lane task, where the robot circulates around a map without making turns. In this week’s efforts, we continued to implement the new follow-route task in Behavior Metrics. This task allows users to evaluate their models on custom pre-defined routes, each with a specified start and end location and distance. The agent must navigate from the start to the end, following high-level commands for correct turns at junctions.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Understand the generation of Behavior Metrics’ evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Implement script mode for the follow-route task</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Implement route loading functionality</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement termination mechanism</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="follow-route-task">Follow-Route Task</h3>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/tasks.png" width="800" />
  </center>
</figure>

<p>This week’s achievements in Behavior Metrics development include:</p>

<ul>
  <li>Enabling the loading of a test suite containing various routes, each defined by the map name, start and end location coordinates, and route length.</li>
  <li>Implementing script mode for running multiple test routes consecutively, recording and outputting evaluation results after each run.</li>
  <li>Detection and recording of traffic light infractions by calculating the actual vehicle turn angles at each junction and comparing them to the provided high-level turning instructions.</li>
</ul>

<p>Moreover, we conducted an in-depth exploration of the codebase to comprehend how existing evaluation metrics are generated after each run. In the upcoming week, we plan to complete the implementation of the follow-route task and expand Behavior Metrics to include new evaluation metrics for this task.</p>

<h3 id="demo">Demo</h3>
<p>In the video below, we demonstrate how script mode is used to run multiple testing episodes sequentially for the follow-route task in Behavior Metrics:</p>
<center><iframe src="https://drive.google.com/file/d/1ZTnsiZsxKkAO2D6yn042ZptfyWrNhf4b/preview" width="640" height="480" allow="autoplay"></iframe></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="BehaviorMetrics" /><category term="EvaluationMetrics" /><summary type="html"><![CDATA[Implement follow-route task in Behavior Metrics]]></summary></entry><entry><title type="html">Week 15: Sep 4 ~ Sep 10</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week15/" rel="alternate" type="text/html" title="Week 15: Sep 4 ~ Sep 10" /><published>2023-09-12T03:32:00-04:00</published><updated>2023-09-12T03:32:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week15</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week15/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week, we continue to work on model enhancement. Despite expanding our dataset, model performance stayed the same. As a result, we opted for experimenting with different architectures. Interestingly, a deeper PilotNet structure didn’t always translate to better outcomes.</p>

<p>Simultaneously, we worked on expanding Behavior Metrics by introducing user customization for task types and integrating additional evaluation metrics.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Test various model architectures.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Add “Follow Lane” and “Follow Route” as different tasks in Behavior Metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Incorporate more evaluation metrics into Behavior Metrics.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Dataset balancing</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="a-deeper-network">A Deeper Network</h3>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/deeper_network.png" width="800" />
  </center>
</figure>

<p>We expanded the PilotNet network with more convolutional and fully connected layers. Yet, the model’s performance remained unchanged. Key results included:</p>
<ul>
  <li>Driving Score:  0.48</li>
  <li>Success Rate: 0.36</li>
  <li>Weighted Success Rate: 0.56</li>
  <li>Infractions:
    <ul>
      <li>Wrong Turn: 12</li>
      <li>Red Light: 0</li>
      <li>Collision w/ Other Vehicle: 7</li>
      <li>Collision w/ Other: 150</li>
      <li>Junction Time Out: 13</li>
      <li>Non-junction Time Out: 7</li>
    </ul>
  </li>
</ul>

<h3 id="behavior-metrics-integration">Behavior Metrics Integration</h3>
<p>Tasks in Behavior Metrics are now categorized as:</p>
<ul>
  <li><strong>FollowLane</strong>: The basic task without turn instructions.</li>
  <li><strong>RandomRoute</strong>: Directions are chosen randomly at intersections.</li>
  <li><strong>FollowRoute</strong>: Route directions are loaded from a .txt file.</li>
</ul>

<p>Users can customize their simulations through the configuration file, as demonstrated below. Moreover, traffic can be toggled on/off based on user settings.</p>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/task_config.png" width="800" />
  </center>
</figure>

<p>We’re also focusing on incorporating more detailed evaluation metrics into Behavior Metrics. Our challenge is determining the simulation’s endpoint. To precisely measure metrics like success rate and driving scores, we must:</p>

<ul>
  <li>Establish defined test routes with start and end points.</li>
  <li>Run the model across numerous episodes.</li>
</ul>

<p>Currently, Behavior Metrics concludes a simulation once the stop button is pressed, without necessarily completing the route. We’re brainstorming ways to seamlessly integrate these new features into Behavior Metrics’ existing framework.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="ImitationLearning" /><category term="BehaviorMetrics" /><summary type="html"><![CDATA[Improving Behavior Metrics Integration]]></summary></entry><entry><title type="html">Week 14: Aug 28 ~ Sep 3</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week14/" rel="alternate" type="text/html" title="Week 14: Aug 28 ~ Sep 3" /><published>2023-09-03T18:31:00-04:00</published><updated>2023-09-03T18:31:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week14</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week14/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week our primary goal was to integrate our current model into Behavior Metrics, completing the entire pipeline for the project. Additionally, we expanded our dataset by training the model on 160 episodes, up from the previous 80. While this typically augments learning, our evaluation results indicated marginal improvements from our prior model.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Update traffic generation PR</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Add more data</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Add model as new brain in Behavior Metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Include more information in evaluation results (e.g. collision per km)</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="training-on-a-larger-dataset">Training on a larger dataset</h3>
<p>More data usually improves learning. This week we increased the dataset to twice the size. Below is the evaluation results for 50 testing episodes. Notably, there wasn’t a significant improvement in terms of overall driving score and succss rate compared to the previous <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week12">combined control model</a> models.</p>
<ul>
  <li>Driving Score: 0.54</li>
  <li>Success Rate: 0.36</li>
  <li>Weighted Success Rate: 0.47</li>
  <li>Infractions:
    <ul>
      <li>Wrong Turn: 18</li>
      <li>Red Light: 1</li>
      <li>Collision w/ Other Vehicle: 10</li>
      <li>Collision w/ Other: 40</li>
      <li>Junction Time Out: 10</li>
      <li>Non-junction Time Out: 4
Below are two video demonstrations - one showcasing a successful run, and the other detailing a failed attempt.</li>
    </ul>
  </li>
</ul>
<center><iframe src="https://drive.google.com/file/d/1C7SDTad4M72MQMx6W3oEu9-BcTQebZYG/preview" width="640" height="480" allow="autoplay"></iframe></center>
<center><iframe src="https://drive.google.com/file/d/1vj7rj2n1WQxRpbEV99RFcPgv1PDRmnTl/preview" width="640" height="480" allow="autoplay"></iframe></center>

<h3 id="behavior-metrics-integration">Behavior Metrics integration</h3>
<p>This week, This week the bulk of the work lies on adding our segmentation-based imitation learning model to BehaviorMetrics as a new brain. Specifically, we added the model as a new brain in Behavior Metrics and the required utilities. 
A <a href="https://github.com/JdeRobot/BehaviorMetrics/pull/632">PR</a> has been created to reflect the following changes:</p>

<ul>
  <li>Added new yaml file, launch file, and object file as example</li>
  <li>Added new brain for loading imitation learning pytorch models</li>
  <li>Added ir_models directory for storing models</li>
  <li>Added utils function for running imitation learning models</li>
  <li>Added PilotNetOneHot model architecture implementation</li>
</ul>

<p>The following is a demonstration where our model gets loaded into Behavior Metrics:</p>
<center><iframe src="https://drive.google.com/file/d/1MxEEB3-spMUPww_hC-CP0jREBPsqShzV/preview" width="640" height="480" allow="autoplay"></iframe></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="ImitationLearning" /><category term="BehaviorMetrics" /><summary type="html"><![CDATA[Behavior Metrics Model Integration]]></summary></entry><entry><title type="html">Week 12~13: Aug 14 ~ Aug 27</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week12/" rel="alternate" type="text/html" title="Week 12~13: Aug 14 ~ Aug 27" /><published>2023-08-27T08:49:00-04:00</published><updated>2023-08-27T08:49:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week12</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week12/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>I have been sick recently so this post briefs the work done from Aug 14 to Aug 27 in the span of two weeks. In the past two weeks, we revisited the challenge of improving our model’s obstacle avoidance. Our prior effort with a weighted L1 loss showed limited progress. The prevailing issue is the model fails to learn to use the brake at all. This week we would like to explore other approaches. One approach involves expanding the dataset with more braking scenarios. We hypothesize that a dataset enriched with varied braking examples might guide the model towards better braking responses. We also plan to experiment with combining the throttle and brake as one model output. At present, our model produces three separate outputs for throttle, steering, and braking, all ranging between [0, 1]. However, a practical observation is that increasing throttle often corresponds with decreased braking, and vice versa. By combining throttle and brake into a single output, we aim to improve the obstacle avoidance capabilities of our model.</p>

<p>Simultaneously, we’re progressing with our Behavior Metrics integration. A priority is to draft a PR for our recently developed traffic generation feature, inviting mentor feedback. Furthermore, if time allows, we hope to lay groundwork for loading our model within Behavior Metrics.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Release posts for weeks 10 and 11</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Expand dataset with more braking scenarios</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Combine throttle and brake as one command</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Draft PR for Behavior Metrics’ traffic generation feature</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="dataset-with-balanced-throttling--braking">Dataset with Balanced Throttling &amp; Braking</h3>
<p>In <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">week 8</a>, we decided to trim the dataset by removing the stationary frames where the vehicle is just waiting in line at a red traffic light. The goal was to make the dataset more compact and consists of only “effective” data, which proved to be a wrong decison. As shown in the left image below, the vehicle is not using the brake (brake = 0) at all in most of the frames from the dataset. The right image shows that after we add more braking data, the amount of time the vehicle is braking vs not braking is roughly balanced.</p>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/brake_dataset_before_vs_after.png" width="800" />
  <figcaption>Before (left) and after (right) adding more braking data</figcaption>
  </center> 
</figure>

<p>After including more braking data in the dataset, we evaluated the model by plotting out the predictions vs ground truth on the training set. As shown in the image blow, the model appears to have learned throttle and steer relatively well yet still struggles with brake. This corroborates with our observation that the model does loose throttle when there is obstacle ahead, but it often fails to utilize the brake. In the Evaluation Results section below, we conduct a more thorough evaluation on the performance of the model.</p>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/balanced_dataset_predictions.png" width="800" />
  </center> 
</figure>

<h3 id="combining-thorttle--brake-control">Combining Thorttle &amp; Brake Control</h3>
<p>Currently our model outputs three vehicle control commands: throttle, steer, and brake. Since the model has not been utilizing the brake after training, we decided to experiment on combining the throttle and brake into one command. Intuitively, the vehicle should either be braking or throttling and not both at the same time. We modify the model such that it outputs two values between [0, 1], one for throttle/brake, and one for steer. For the throttle/brake command, we threshold it at 0.5 such that if it’s above the threshold, the vehivle hits thorttle, and vice versa.</p>

<p>\(throttle = (combined\_control - 0.5) / 0.5 \quad \text{if combined_control &gt;= 0.5}\)
\(brake = (0.5 - combined\_control) / 0.5 \quad \text{if combined_control &lt;0.5}\)</p>

<p>As shown in the image below, the model has learned brake well as most predictions are close to the ground truth. However, it still tends to over-use the throttle and under-utilize the brake.</p>
<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/combined_control_predictions.png" width="700" />
  </center> 
</figure>

<h3 id="evaluation-results">Evaluation Results</h3>
<p>We compare the following models:</p>
<ol>
  <li><strong>Unbalanced Dataset</strong>: model from our last iteration that was trained on an unbalanced dataset</li>
  <li><strong>Balanced Dataset</strong>: same as 1 except the brake values in dataset is balanced</li>
  <li><strong>Balanced Dataset + Additional Junction Data</strong>: same as 2 except with 50 additional episodes of the expert agent crossing a junction</li>
  <li><strong>Balanced Dataset + Combined Control</strong>: combines throttle and brake into a single command</li>
  <li><strong>Balanced Dataset + Combined Control + Additional Junction Data</strong>: same as 4 except with 50 additional episodes of the expert agent crossing a junction</li>
</ol>
<center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/v9_evaluation results.png" width="800" />
  </center>
<p>From the evaluation results above, we make the following observations:</p>
<ul>
  <li>Compared to the model trained on the unbalanced dataset, the models trained on balanced dataset are better at obstacle avoidance in terms of decreased nubmer of collisions with other vehicles.</li>
  <li>Despite the improved performance in obstacle avoidance, the overall success rate decreased drastically. Specifically, the new models frequently get stuck in a state and time out eventually. This might be because the model is being overly cautious and drives too slowly overall. The agent might be over-predicting the necessity to brake due to ambiguous situations or over-representing the braking action in the dataset.</li>
  <li>Counterintuitively, the additional 50 episodes of “junction data” doesn’t improve the learning. Notably, the number of collisions increased significantly (model 3 verus 2, 5 verus 4). It’s possible that the behavior in the junctions is conflicting with behavior in other parts of the driving task. This can lead to situations where the agent is confused about which behavior to imitate.</li>
</ul>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="ImitationLearning" /><category term="BehaviorMetrics" /><summary type="html"><![CDATA[Training with balanced data and exploring combined throttle-brake model output]]></summary></entry><entry><title type="html">Week 11: Aug 07 ~ Aug 13</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week11/" rel="alternate" type="text/html" title="Week 11: Aug 07 ~ Aug 13" /><published>2023-08-15T06:00:00-04:00</published><updated>2023-08-15T06:00:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week11</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week11/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week, our focus remained on enhancing our model’s performance, especially in the area of obstacle avoidance. Despite the agent’s attempts to decelerate when detecting a vehicle ahead, it doesn’t employ the brake efficiently, leading to frequent collisions. To address this, we adjusted the L1 loss used during training, giving more emphasis to braking than to throttle or steering. We also made progress in integrating traffic generation functionality within Behavior Metrics.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Adjust driving score weights</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Modify the L1 loss weights for training</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Adjust the model-autopilot driving balance for DAgger data collection</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Continue integration with Behavior Metrics platform</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="weighted-l1-score">Weighted L1 Score</h3>
<p>Our primary concern is the agent’s inadequate braking response. The image below, showing ground truth versus model predictions on the validation set, underscores this deficiency. Note that the model’s predicted braking remains at zero regardless of the true label value, suggesting a failure to grasp appropriate braking instances.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/unweighted L1 loss.png" width="800" /></center>
<p>Previously, our training has been using the L1 loss. In an effort to better capture braking nuances, we shifted to a weighted L1 loss, assigning weights of 2:1:5 to throttle, steering, and braking, respectively. The intention was to encourage the model to prioritize learning about braking. However, as shown below, this alteration had minimal impact on the model’s learning pattern.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/weighted L1 loss.png" width="800" /></center>

<h3 id="other-ideas-auxiliary-tasks">Other Ideas: Auxiliary Tasks</h3>
<p>As mentioned above, our effort to improve the model’s performance in obstacle avoidance by reweighing the L1 loss didn’t quite work. This persistent issue suggests that we may need to re-evaluate our data, consider other model architectures, or explore alternative training strategies. One of the ideas we have is to train with auxiliary tasks. Here are some auxiliary tasks we might consider:</p>
<ul>
  <li><strong>Braking Prediction</strong>: By making the model anticipate when it should brake, it could enhance its real-time braking responses.</li>
  <li><strong>Obstacle Distance</strong> Estimation: Predicting the distance to the nearest obstacle could help in modulating speed and deciding when to brake.</li>
  <li><strong>Speed Prediction</strong>: By predicting its next speed, the model might better adjust throttle and braking actions based on upcoming conditions.</li>
</ul>

<h3 id="traffic-generation-in-behavior-metrics">Traffic Generation in Behavior Metrics</h3>
<p>The user can now specify the number of other vehicles and pedestrians while configuring for the scenario in Behavior Metrics using the yaml config file. An example showcased below demonstrates a scenario where the street is populated by other vehicles. Importantly, these vehicles are set in motion only once the user initiates the ‘start’ and will halt once the agent stops. Switching from asynchronous to synchronous mode in CARLA facilitates this synchronization. We will shortly open a PR for this feature, inviting feedback before finalizing the update.</p>
<center><iframe src="https://drive.google.com/file/d/1KLEe_6CdB6pE0ndbEaZSgRp2orz6i8LZ/preview" width="560" height="400" allow="autoplay"></iframe></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="ImitationLearning" /><category term="BehaviorMetrics" /><summary type="html"><![CDATA[Exploring weighted L1 loss and integration with Behavior Metrics]]></summary></entry><entry><title type="html">Week 10: July 31 ~ Aug 06</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week10/" rel="alternate" type="text/html" title="Week 10: July 31 ~ Aug 06" /><published>2023-08-07T06:00:00-04:00</published><updated>2023-08-07T06:00:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week10</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week10/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week pivots towards furthur enhancing and evaluating the model. With the previously implemented evaluation metrics, we can now more thoroughly gauge our model’s performance. As the program enters its latter half, we would like to start transferring the work we have done so far to the <a href="https://jderobot.github.io/BehaviorMetrics/">Behavior Metrics</a> platform. Tailored for benchmarking autonomous driving models, this platform allows users to load their models into predefined scenarios and obtain a detailed performance metric report. Currently, the platform only supports the lane-following task, where the agent doesn’t have to deal with obstacles (other vehicles and pedestrians), intersections, or traffic lights. An immediate goal for this week is to expand Behavior Metrics by introducing traffic, thus making its environment more dynamic and realistic.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Release posts for weeks 7 and 8</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Update codebase</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Refine and assess model performance</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Begin integration with Behavior Metrics platform</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="model-evaluation">Model Evaluation</h3>
<p>One intriguing finding from the evaluation is what appears to be an inverse relationship between success rate and the overall driving score.
As shown in the image below, after five dagger iterations, the success rate is the highest but the overall driving score is the lowest.  Let’s dissect the potential rationale:</p>

<p>The model is bad at obstacle avoidance. When the model strays from the designated path (for instance, by taking an incorrect turn), it invariably ends up with a subpar route completion score, denoted as \(R\). However, this deviation inadvertently minimizes its exposure to potential obstacles. On the contrary, when the agent actually drives all the way to the target location, it is more likely to collide with
other vehicles, resulting in a significanlty lower infraction penalty score \(P\), contributing to a diminished overall driving score, \(PR\).</p>

<p>Another issue is once a collision with a front vehicle occurs, the agent is likely to repeatedly bump into the same vehicle as they both move forward staying very closely to each other, causing the infraction penalty score to decrease <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week6">exponentially</a>.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v8_evaluation results.png" /></center>

<h3 id="v824-demo">v8.2.4 Demo</h3>
<p>As shown in the demo below, after five DAgger iterations, the agent is able to follow the lane and follow instructions to make turns without much trouble, but lacks in avoiding obstacles, which we will work on in the 
upcoming weeks.</p>
<center><iframe src="https://drive.google.com/file/d/1xAIVwNmU8DCRFAoRrKAMalsQsR66L7cK/preview" width="560" height="400" allow="autoplay"></iframe></center>

<h3 id="traffic-generation-in-behavior-metrics">Traffic Generation in Behavior Metrics</h3>
<p>This week, we began working on integrating traffic generation for CARLA tasks in Behavior Metrics. For updates on this, check the <a href="https://github.com/JdeRobot/BehaviorMetrics/issues/625">issue</a> we opened on the Behavior Metrics Github page.</p>

<p>We had some difficulty with generating pedestrians. An issue arises with the <a href="https://github.com/deepsense-ai/carla-birdeye-view">carla-birdeye-view</a> dependency of Behavior Metrics. When this tool tries to create a bird’s eye view for pedestrians, it runs into a NaN error. This happens because the tool mistakenly identifies the AI controllers used by pedestrians as pedestrians themselves, leading to errors when it tries to read their bounding boxes. We’ve reported this on the carla-birdeye-view Github page.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="BehaviorMetrics" /><summary type="html"><![CDATA[Diving into model evaluation and Behavior Metrics integration.]]></summary></entry><entry><title type="html">Week 9: July 24 ~ July 30</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9/" rel="alternate" type="text/html" title="Week 9: July 24 ~ July 30" /><published>2023-07-31T04:39:00-04:00</published><updated>2023-07-31T04:39:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week, our focus was on enhancing the performance of the model, particularly in addressing a prevalent issue identified in the previous week: the model would suddenly halt and become stuck in a state, especially when it is crossing an intersection. We tackled this problem using two main approaches:</p>
<ul>
  <li>Finetuning the model with more junction data.</li>
  <li>Use DAgger (Data Aggregation) to iteratively improve the model.</li>
</ul>

<p>In addition, we faced a problem with our current evaluation metrics: they do not detect whether the model is correctly following turning instructions. During an evaluation episode, if the vehicle makes a wrong turn, calculating the percentage of the route completed becomes meaningless since it deviates from the intended route. Thus, it’s necessary to implement a mechanism to detect which turn the vehicle actually made. This way, we can terminate the testing episode if an incorrect turn is made and proceed to the next one.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Refine the evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Continue improving model (finetuning, DAgger)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish setting up Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="current-models">Current Models</h3>
<p>This week we trained and compared three models:</p>

<ul>
  <li><strong>v8.0</strong>: vanilla model trained on 80 episodes of training data</li>
  <li><strong>v8.1</strong>: v8.0 finetuned on 80 additional junction data episodes</li>
  <li><strong>v8.2</strong>: v8.0 retrained with DAgger for 3 iterations</li>
</ul>

<h3 id="baseline---v80">Baseline - v8.0</h3>
<p>In the video below, the agent is able to roam the streets without suddenly stopping. However, we can still observe several problems. 
(1) It fails to make a left turn at around 00:40.
(2) Starting from 1:15, the agent drifts to the left lane, potentially causing a collision if there were other vehicles around.</p>
<center><iframe src="https://drive.google.com/file/d/1fu0glSpZwSh89i51v1D0u4ujSK1jWc24/preview" width="560" height="400" allow="autoplay"></iframe></center>

<h3 id="modified-dagger-implementation">(Modified) DAgger Implementation</h3>
<p>One of the challenges this week was to implement DAgger. As <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">previously</a> described, DAgger involves running the trained agent in the simulator, with the expert on the side.  This allows the expert to demonstrate in more diverse scenearios that the agent might realistically encounter. While the agent drives the car, we record the actions the expert would have taken for each state and add this extra data to our dataset to retrain the model.
However, using the CARLA traffic manager as the expert presents a challenge, as there is no way to “query” the expert while allowing the agent to drive the vehicle. To address this, we implemented DAgger in the following way:</p>

<ul>
  <li>Train an agent using the initial dataset.</li>
  <li>Allow both the agent and the expert to drive the car in the simulator, <strong>alternating every 20 frames</strong>.</li>
  <li>Only record the actions taken by the expert, not the agent.</li>
  <li>Incorporate the new data into the training set and retrain the agent.</li>
  <li>Repeat Process, iteratively refining the agent’s performance.</li>
</ul>

<p>The image below demonstrates a contrasting situation where the expert and the model choose different actions during DAgger data collection. In this specific scenario, the agent (model) opts to accelerate, likely because the traffic light is green. Meanwhile, the expert seizes the opportunity to demonstrate the “correct” action by applying the brake, recognizing the presence of another vehicle ahead. This example highlights the valuable corrections that the expert can provide in the DAgger process, guiding the model towards more sophisticated decision-making in complex scenarios.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/DAgger_data collection.png" /></center>

<h3 id="turning-detection">Turning Detection</h3>
<p>During evaluation, the model is tested on a set of routes with predefined starting and target locations, as well as a sequence of turning instructions. In order to better 
evaluate the model, we developed a mechanism to accumulately calculate the angle turned at a junction, as shown in the picture below. The idea is to record the changes in the yaw of the vehicle upon entering a junction. When the evluation program detects a failed or wrong turn, the episode ends and the testing moves on to the next one.</p>

<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/turning_angle_calculation.png" width="400" /></center>

<h3 id="demo-822">Demo 8.2.2</h3>
<p>The video below demonstrates the model refined by DAgger for three times. For testing its ability to follow the lane and navigate intersections, no obstacles are spawned and the traffic lights are set to green.</p>
<center><iframe src="https://drive.google.com/file/d/1Fhhggpjm8W3gMyEViL7gaJ8C1GO2Vmwt/preview" width="560" height="400" allow="autoplay"></iframe></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="Finetuning" /><category term="DAgger" /><summary type="html"><![CDATA[Using Data Aggregation(DAgger) for iterative improvement, and implementing a refined mechanism for turn detection in evaluations.]]></summary></entry><entry><title type="html">Week 8: July 17 ~ July 23</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8/" rel="alternate" type="text/html" title="Week 8: July 17 ~ July 23" /><published>2023-07-24T04:47:00-04:00</published><updated>2023-07-24T04:47:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week we focused on addressing the “halting” problem in the latest model we observed from last week. The problem occurs the most frequently when the vehicle is making a turn at intersections where they would suddenly stop in the middle of the road and get stuck in that state.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Experiment with adding more meaningful data, adding more modalities etc to address the halting problem</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish installing Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="efficient-data-collection">Efficient data collection</h3>
<p>The solution to most problems in the realm of machine learning lies in more data. In research settings, autonomous driving tasks often require dozens to hundreds of hours of driving data. Given computational limitations, we’re currently collecting under 100 episodes of training data in the CARLA simulator, amassing a few hours of driving at most. Therefore, we need to make our data collection process as efficient and effective as possible. This week, we explored the following ways to optimize data collection:</p>
<ul>
  <li><strong>Data Trimming</strong>: A typical episode in the training set usually involves the agent waiting at the intersection for a long time. We removed segments where the vehicle stops and the throttle remains below 0.01 after more than 50 frames. This step helps eliminate redundant data and increases the effectiveness of our dataset. (<strong>Update: NOT a good idea</strong>, see <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week12">week 12</a>)</li>
  <li><strong>Data prioritization</strong>: Certain tasks, such as making turns at intersections and obstacle avoidance, are more complex and crucial. Therefore, we should prioritize these scenarios in our data collection.</li>
  <li><strong>Selective Data Retention</strong>: After the initial training, we can analyze the scenarios where the model underperforms, and focus on collecting and retaining more data that reflects these scenarios.</li>
</ul>

<h3 id="finetuning-on-junction-only-dataset">Finetuning on Junction-Only Dataset</h3>
<p>In previous week, our observation was that the model often stops suddenly in the middle of making a turn at an intersection. By collecting additional episodes that specifically targeted junctions, we were able to create a “junction-only” dataset. This allowed us to finetune the model to hopefully better learn the nuances of navigating intersections from the additional data. The image below illustrates a few examples of the data collected.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/junctions_data_collection.gif" width="400" /></center>

<p>The video below demonstrates an example of vehicle successfully making a left turn after finetuning.</p>
<center><iframe src="https://drive.google.com/file/d/1HGh6hiu3nq49x0F14oAKhwQQtC0nIPuZ/preview" width="560" height="400" allow="autoplay"></iframe></center>
<p>However, even after finetuning. Some problems persist. The next video shows a failed example where the vehicle goes straight at an intersection despite the “Turn Left” command.</p>
<center><iframe src="https://drive.google.com/file/d/1zLqXFHLEUZsSmiFUDuwplV_PmJbhexcF/preview" width="560" height="400" allow="autoplay"></iframe></center>

<p>Overall, there is some improvement and the agent seems to get stuck less at intersections after finetuning. To truly evaluate the effectiveness of the finetuning, especially in terms of the success rate of making turns, we need to develop a mechanism for detecting whether the agent successfully makes a turn or not, which is lacking in our current evaluation process. (<strong>Update</strong>: turning detection implemented in <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week9">week 9</a>)</p>

<h3 id="data-aggregation-dagger">Data Aggregation (DAgger)</h3>
<p>Another technique that we plan to try in the following week is called Data Aggregation(DAgger)[1]. DAgger is an iterative algorithm designed to improve the performance of machine learning models, particularly imitation learning. Traditional imitation learning can sometimes lead to a problem known as “distributional shift,” where a trained model, when put into practice, might encounter states that were not well-represented in the initial training data. This discrepancy can result in suboptimal or incorrect actions by the model.</p>

<p>DAgger addresses this issue by iteratively training the model with a combination of its own behavior and expert guidance. By repeatedly collecting the states that the policy experiences and querying an expert for the correct actions, DAgger continually refines the training dataset and the policy. This approach helps the model to generalize better to unseen states, aligning more closely with the expert’s performance.</p>

<p>In the context of our project, the steps to apply DAgger can be outlined as follows:</p>

<ol>
  <li>Initialize dataset D with autopilot expert demonstrations</li>
  <li>Train policy on D</li>
  <li>Run trained policy in the simulator: only save the states that the policy sees, not the actions it takes.</li>
  <li>Ask the expert for actions: For each state in the newly generated trajectory, query the expert what action they would take</li>
  <li>Add the new state-action pairs to D</li>
  <li>Retrain policy on D</li>
  <li>Repeat steps 3-6 for a number of iterations</li>
</ol>

<h2 id="references">References</h2>
<p>[1] Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Conference on Artificial Intelligence and Statistics (AISTATS), 2011.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="Finetuning" /><summary type="html"><![CDATA[Addressing the model's "halting" issue at intersections through specialized data collection and finetuning]]></summary></entry></feed>