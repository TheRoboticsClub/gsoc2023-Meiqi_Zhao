<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-08-04T17:11:07-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/feed.xml</id><title type="html">Meiqi Zhao | JdeRobot x GSoC2023</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Week 9: July 24 ~ July 30</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9/" rel="alternate" type="text/html" title="Week 9: July 24 ~ July 30" /><published>2023-07-31T04:39:00-04:00</published><updated>2023-07-31T04:39:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week9/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week, our focus was on enhancing the performance of the model, particularly in addressing a prevalent issue identified in the previous week: the model would suddenly halt and become stuck in a state, especially when it is crossing an intersection. We tackled this problem using two main approaches:</p>
<ul>
  <li>Finetuning the model with more junction data.</li>
  <li>Use DAgger (Data Aggregation) to iteratively improve the model.</li>
</ul>

<p>In addition, we faced a problem with our current evaluation metrics: they do not detect whether the model is correctly following turning instructions. During an evaluation episode, if the vehicle makes a wrong turn, calculating the percentage of the route completed becomes meaningless since it deviates from the intended route. Thus, it’s necessary to implement a mechanism to detect which turn the vehicle actually made. This way, we can terminate the testing episode if an incorrect turn is made and proceed to the next one.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Refine the evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Continue improving model (finetuning, DAgger)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish setting up Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="current-models">Current Models</h3>
<p>This week we trained and compared three models:</p>

<ul>
  <li><strong>v8.0</strong>: vanilla model trained on 80 episodes of training data</li>
  <li><strong>v8.1</strong>: v8.0 finetuned on 80 additional junction data episodes</li>
  <li><strong>v8.2</strong>: v8.0 retrained with DAgger for 3 iterations</li>
</ul>

<h3 id="baseline---v80">Baseline - v8.0</h3>
<p>In the video below, the agent is able to roam the streets without suddenly stopping. However, we can still observe several problems. 
(1) It fails to make a left turn at around 00:40.
(2) Starting from 1:15, the agent drifts to the left lane, potentially causing a collision if there were other vehicles around.</p>
<center><iframe src="https://drive.google.com/file/d/1fu0glSpZwSh89i51v1D0u4ujSK1jWc24/preview" width="560" height="400" allow="autoplay"></iframe></center>

<h3 id="modified-dagger-implementation">(Modified) DAgger Implementation</h3>
<p>One of the challenges this week was to implement DAgger. As <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">previously</a> described, DAgger involves running the trained agent in the simulator, with the expert on the side.  This allows the expert to demonstrate in more diverse scenearios that the agent might realistically encounter. While the agent drives the car, we record the actions the expert would have taken for each state and add this extra data to our dataset to retrain the model.
However, using the CARLA traffic manager as the expert presents a challenge, as there is no way to “query” the expert while allowing the agent to drive the vehicle. To address this, we implemented DAgger in the following way:</p>

<ul>
  <li>Train an agent using the initial dataset.</li>
  <li>Allow both the agent and the expert to drive the car in the simulator, <strong>alternating every 20 frames</strong>.</li>
  <li>Only record the actions taken by the expert, not the agent.</li>
  <li>Incorporate the new data into the training set and retrain the agent.</li>
  <li>Repeat Process, iteratively refining the agent’s performance.</li>
</ul>

<p>The image below demonstrates a contrasting situation where the expert and the model choose different actions during DAgger data collection. In this specific scenario, the agent (model) opts to accelerate, likely because the traffic light is green. Meanwhile, the expert seizes the opportunity to demonstrate the “correct” action by applying the brake, recognizing the presence of another vehicle ahead. This example highlights the valuable corrections that the expert can provide in the DAgger process, guiding the model towards more sophisticated decision-making in complex scenarios.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/DAgger_data collection.png" /></center>

<h3 id="turning-detection">Turning Detection</h3>
<p>During evaluation, the model is tested on a set of routes with predefined starting and target locations, as well as a sequence of turning instructions. In order to better 
evaluate the model, we developed a mechanism to accumulately calculate the angle turned at a junction, as shown in the picture below. The idea is to record the changes in the yaw of the vehicle upon entering a junction. When the evluation program detects a failed or wrong turn, the episode ends and the testing moves on to the next one.</p>

<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/turning_angle_calculation.png" width="400" /></center>

<h3 id="demo-822">Demo 8.2.2</h3>
<p>The video below demonstrates the model refined by DAgger for three times. For testing its ability to follow the lane and navigate intersections, no obstacles are spawned and the traffic lights are set to green.</p>
<center><iframe src="https://drive.google.com/file/d/1Fhhggpjm8W3gMyEViL7gaJ8C1GO2Vmwt/preview" width="560" height="400" allow="autoplay"></iframe></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="Finetuning" /><category term="DAgger" /><summary type="html"><![CDATA[Using Data Aggregation(DAgger) for iterative improvement, and implementing a refined mechanism for turn detection in evaluations.]]></summary></entry><entry><title type="html">Week 8: July 17 ~ July 23</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8/" rel="alternate" type="text/html" title="Week 8: July 17 ~ July 23" /><published>2023-07-24T04:47:00-04:00</published><updated>2023-07-24T04:47:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week8/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week we focused on addressing the “halting” problem in the latest model we observed from last week. The problem occurs the most frequently when the vehicle is making a turn at intersections where they would suddenly stop in the middle of the road and get stuck in that state.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Experiment with adding more meaningful data, adding more modalities etc to address the halting problem</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish installing Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="efficient-data-collection">Efficient data collection</h3>
<p>The solution to most problems in the realm of machine learning lies in more data. In research settings, autonomous driving tasks often require dozens to hundreds of hours of driving data. Given computational limitations, we’re currently collecting under 100 episodes of training data in the CARLA simulator, amassing a few hours of driving at most. Therefore, we need to make our data collection process as efficient and effective as possible. This week, we explored the following ways to optimize data collection:</p>
<ul>
  <li><strong>Data Trimming</strong>: A typical episode in the training set usually involves the agent waiting at the intersection for a long time. We removed segments where the vehicle stops and the throttle remains below 0.01 after more than 50 frames. This step helps eliminate redundant data and increases the effectiveness of our dataset.</li>
  <li><strong>Data prioritization</strong>: Certain tasks, such as making turns at intersections and obstacle avoidance, are more complex and crucial. Therefore, we should prioritize these scenarios in our data collection.</li>
  <li><strong>Selective Data Retention</strong>: After the initial training, we can analyze the scenarios where the model underperforms, and focus on collecting and retaining more data that reflects these scenarios.</li>
</ul>

<h3 id="finetuning-on-junction-only-dataset">Finetuning on Junction-Only Dataset</h3>
<p>In previous week, our observation was that the model often stops suddenly in the middle of making a turn at an intersection. By collecting additional episodes that specifically targeted junctions, we were able to create a “junction-only” dataset. This allowed us to finetune the model to hopefully better learn the nuances of navigating intersections from the additional data. The image below illustrates a few examples of the data collected.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/junctions_data_collection.gif" width="400" /></center>

<p>The video below demonstrates an example of vehicle successfully making a left turn after finetuning.</p>
<center><iframe src="https://drive.google.com/file/d/1HGh6hiu3nq49x0F14oAKhwQQtC0nIPuZ/preview" width="560" height="400" allow="autoplay"></iframe></center>
<p>However, even after finetuning. Some problems persist. The next video shows a failed example where the vehicle goes straight at an intersection despite the “Turn Left” command.</p>
<center><iframe src="https://drive.google.com/file/d/1zLqXFHLEUZsSmiFUDuwplV_PmJbhexcF/preview" width="560" height="400" allow="autoplay"></iframe></center>

<p>Overall, there is some improvement and the agent seems to get stuck less at intersections after finetuning. To truly evaluate the effectiveness of the finetuning, especially in terms of the success rate of making turns, we need to develop a mechanism for detecting whether the agent successfully makes a turn or not, which is lacking in our current evaluation process. (<strong>Update</strong>: turning detection implemented in <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week9">week 9</a>)</p>

<h3 id="data-aggregation-dagger">Data Aggregation (DAgger)</h3>
<p>Another technique that we plan to try in the following week is called Data Aggregation(DAgger)[1]. DAgger is an iterative algorithm designed to improve the performance of machine learning models, particularly imitation learning. Traditional imitation learning can sometimes lead to a problem known as “distributional shift,” where a trained model, when put into practice, might encounter states that were not well-represented in the initial training data. This discrepancy can result in suboptimal or incorrect actions by the model.</p>

<p>DAgger addresses this issue by iteratively training the model with a combination of its own behavior and expert guidance. By repeatedly collecting the states that the policy experiences and querying an expert for the correct actions, DAgger continually refines the training dataset and the policy. This approach helps the model to generalize better to unseen states, aligning more closely with the expert’s performance.</p>

<p>In the context of our project, the steps to apply DAgger can be outlined as follows:</p>

<ol>
  <li>Initialize dataset D with autopilot expert demonstrations</li>
  <li>Train policy on D</li>
  <li>Run trained policy in the simulator: only save the states that the policy sees, not the actions it takes.</li>
  <li>Ask the expert for actions: For each state in the newly generated trajectory, query the expert what action they would take</li>
  <li>Add the new state-action pairs to D</li>
  <li>Retrain policy on D</li>
  <li>Repeat steps 3-6 for a number of iterations</li>
</ol>

<h2 id="references">References</h2>
<p>[1] Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Conference on Artificial Intelligence and Statistics (AISTATS), 2011.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="Finetuning" /><summary type="html"><![CDATA[Addressing the model's "halting" issue at intersections through specialized data collection and finetuning]]></summary></entry><entry><title type="html">Week 7: July 10 ~ July 16</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week7/" rel="alternate" type="text/html" title="Week 7: July 10 ~ July 16" /><published>2023-07-24T04:30:00-04:00</published><updated>2023-07-24T04:30:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week7</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week7/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>This week we continue to improve the performance of our model. One major problem we faced this week is the problem of the agent occasionally stopping and getting stuck in a state, which we call the “halting” problem. Understanding and addressing this issue became a central focus, leading us to think about the underlying circumstances and potential solutions. Furthermore, we have also explored the model’s response to traffic lights and conducted cross-town testing to understand generalization.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Complete midterm evaluation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish training and evaluating v7.0, 7.1</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Confirm whether the model needs a larger amount of data</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Read literature on the halting problem</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Continue reading codebase of Behavior Metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Integrate the existing evaluation metrics into Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="current-models">Current Models</h3>
<p>Below is a summary of our current models and their performance. As shown in the first table, using one-hot encoding vs an embedding layer for the high level command does not have a big impact on the model in terms of overall performance (<a href="/gsoc2023-Meiqi_Zhao/blog/2023/week8">How is driving score calculated?</a>). The second table compares model v7.1 and v7.2, the former ignores the traffic light status and the latter is trained with the traffic light as an additional input to the model. The results show that 
just by passing the status of the traffic light (Not at traffic light, Red, Green, Yellow) as a one-hot encoding, the model is able to effectively follow the traffic light in testing.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v7.0_vs_v7.1.png" width="500" /></center>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v7.1_vs_v7.2.png" width="500" /></center>

<h3 id="testing-in-town01-vs-town02">Testing in Town01 vs Town02</h3>
<p>So far we have only been training our model in Town01 in CARLA and testing in a different town, Town02. In order to verify whether the model can benefit from a larger amount of data, we tested the model in Town01 vs in Town02. As expected, as shown in the table below, the model performs significantly worse in unseen environmenet. This indicates that if we want to improve the model’s generalization to different environments, we must consider expanding the training data to include various towns, roads, and scenarios. One of the conclusions drawn from this experiment is that a model trained exclusively on Town01 data is overfitting to that particular environment. While it may perform admirably in familiar surroundings, its performance degrades when presented with unseen or novel situations.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/Town01_vs_Town02.png" width="400" /></center>

<h3 id="the-halting-problem">The “Halting” Problem</h3>
<p>One of the persistent challenges we encountered this week is what we term the “halting” problem. This issue is characterized by the agent suddenly stopping in the middle of a task, getting stuck in a particular state and unable to continue its course.  To understand this problem more deeply, we began to analyze the specific circumstances under which the halting occurs. We made the following observations:</p>

<ol>
  <li>Although this issue can surface in a variety of situations, it is most commonly observed when the vehicle is navigating turns at intersections.</li>
  <li>While the vehicle is immobilized, we noted that the throttle value hovers close to 0.0, whereas the brake value often exceeds 0.9. Interestingly, when the agent perceives another vehicle passing by, it releases the brake and begins to accelerate. However, this short-lived acceleration fails to significantly alter the vehicle’s position, leaving it stagnant once more.</li>
</ol>

<p>The image below illustrates two typical instances of the “halting” issue, where the vehicle becomes stuck without apparent cause.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/halting.png" width="800" /></center>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><summary type="html"><![CDATA[Exploring model improvements, focusing on the "halting" problem and generalization.]]></summary></entry><entry><title type="html">Week 5: June 26 ~ July 02</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week5/" rel="alternate" type="text/html" title="Week 5: June 26 ~ July 02" /><published>2023-07-09T13:07:00-04:00</published><updated>2023-07-09T13:07:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week5</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week5/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>Our current model employs an embedding layer to process the high-level commands, which are then concatenated with the flattened visual features. However, given that we only have four discrete high-level commands, the use of an embedding layer might be overcomplicated for our scenario. Therefore, we plan to experiment with one-hot encoding for these high-level commands this week.</p>

<p>Until now, our model has ignored traffic lights as the expert agent was configured to do so during data collection. However, we believe incorporating traffic light status as an additional input to our model would make our agent more compliant with real-world traffic regulations.</p>

<p>With the approaching midterm evaluation and the significant progress we’ve made so far, we are also considering creating a demo video of our work. We aim to showcase our advancements on our <a href="https://www.youtube.com/@jderobot/videos">Youtube Channel</a>. Moreover, we plan to update our code repository and compose a comprehensive README to provide better guidance for those interested in our project.
and write a README.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Replace the embedding layer for high-level commands with one-hot encoding</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Introduce traffic light status as an additional input to the model</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Write a README for repository</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Produce a demo video for the JdeRobot Youtube Channel</li>
</ul>

<h2 id="execution">Execution</h2>

<h3 id="one-hot-encoding-for-high-level-command">One-Hot Encoding for High-Level Command</h3>
<p>Below is the diagram illustrating the revised model architecture which now processes high-level commands using one-hot encoding. Surprisingly, this adjustment seems to have negatively impacted the model’s performance. The agent now occasionally collides with static objects while executing a turn. This suggests that the agent, using the one-hot encoded commands, hasn’t learned the task of making turns as effectively as it did when the embedding layer was used. This result is counterintuitive, given that one-hot encoding generally presents simpler, more explicit information for the model to learn, thus eliminating the need for the model to learn the weights for an embedding layer.</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v6.3_model architecture.png" width="600" /></center>
<iframe src="https://drive.google.com/file/d/1U2huVDduwzhtiGYpthJKDAlwDJUTX74g/preview" width="800" height="450" allow="autoplay"></iframe>

<h3 id="transition-to-carla-0914">Transition to CARLA 0.9.14</h3>
<p>This week, we also initiated the transition from CARLA 0.9.13 to 0.9.14. The main motivations for this change were as follows:</p>

<ul>
  <li>In CARLA 0.9.14, we can set the target speed of an autopilot agent via the Traffic Manager, thus allowing us to maintain a constant speed for the expert agent.</li>
  <li>CARLA 0.9.14 allows for differentiation between various types of vehicles through semantic segmentation.</li>
  <li>In the latest version, we can retrieve the traffic light status for each frame.</li>
</ul>

<p>However, this transition wasn’t entirely smooth. An issue we ran into was that CARLA updated its semantic segmentation labels in version 0.9.14 to align with the Cityscape dataset. Consequently, our older models, for which data was collected from version 0.9.13, can no longer be used for evaluation moving forward.</p>

<h3 id="traffic-light-status-as-model-input">Traffic Light Status As Model Input</h3>
<p>The image below illustrates the architecture where we’ve added traffic light status as an additional input modality. The status of the traffic light can take one of four discrete values, encoded using one-hot encoding:</p>
<ul>
  <li><strong>0</strong> Not at traffic light</li>
  <li><strong>1</strong> Red</li>
  <li><strong>2</strong> Green</li>
  <li><strong>3</strong> Yellow</li>
</ul>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v6.4_model architecture.png" width="600" /></center>
<p>As can be seen in the video below, the model learns to stop at traffic lights. Please note that the video demonstrates successful cases primarily. The overall performance of the model, however, is not yet optimal, as it frequently runs red lights. We will continuously workto improve this aspect and to make our model more accurate in responding to traffic signals.</p>
<iframe src="https://drive.google.com/file/d/1Bv1x-BFap-OQSNPYq4br505vj7F-7nJG/preview" width="800" height="450" allow="autoplay"></iframe>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><summary type="html"><![CDATA[Enhancing agent's traffic light adherence]]></summary></entry><entry><title type="html">Week 6: July 03 ~ July 09</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week6/" rel="alternate" type="text/html" title="Week 6: July 03 ~ July 09" /><published>2023-07-09T13:07:00-04:00</published><updated>2023-07-09T13:07:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week6</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week6/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>The main objective for this week is to develop more sophisticated evaluation metrics beyond the existing success rate and average distance traveled before a collision. Specifically, we intend to incorporate metrics from the <a href="https://leaderboard.carla.org/">CARLA Leaderboard</a> into our evaluation process, which take into account route completion and traffic infractions when calculating a score for model performance.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish editing the demo video</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Develop advanced evaluation metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Debug CARLA 0.9.14 segmentation label issue</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Study evaluation metrics in Behavior Metrics</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="current-models">Current models</h3>
<p>Following the upgrade to CARLA 0.9.14, due to the changes in the semantic segmentation labels, we had to retrain our previous models which were initially trained under CARLA 0.9.13. Below is a summary of the current state of our models (see <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week5">previous post</a> for model architectures):</p>
<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/v7_models.png" width="400" /></center>
<p>By comparing v7.0 and v7.1, we aim to discern whether one-hot encoding or an embedding layer leads to superior performance. Furthermore, by comparing v7.1 and v7.2 and specifically observing the number of traffic light infractions, we can evaluate how effectively model v7.2 has learned to adhere to traffic light signals.</p>

<h3 id="evaluation-metrics">Evaluation Metrics</h3>
<p>Following the scoring system of the CARLA Leaderboard, we have updated our evaluation metrics as follows:</p>
<ul>
  <li><strong>Driving Score</strong>: \(R_i P_i\) where \(R_i\) represents the percentage of completion of the \(i\)-th route and \(P_i\) the infraction penalty of the \(i\)−th route.</li>
  <li><strong>Route Completion</strong>: This metric represents the percentage of the route completed. For each testing route, we pre-calculate the total length of the route by letting an expert agent, i.e., the CARLA autopilot, drive through the route and measure the distance traveled from start to end.</li>
  <li><strong>Infraction Penalty</strong>: \(\prod_j (p_i ^j)^{\text{# of infractions}_j}\) where \(p^j\) is the penalty of infractions of type \(j\) as specified below:
    <ul>
      <li>Collision with walker: <strong>0.5</strong></li>
      <li>Collision with other vehicle: <strong>0.6</strong></li>
      <li>Collision with static objects: <strong>0.65</strong></li>
      <li>Timeout: <strong>0.7</strong></li>
      <li>Running a red light: <strong>0.7</strong>
The overall driving score will be the arithmetic mean of the driving scores of all routes.</li>
    </ul>
  </li>
</ul>

<h3 id="demo-video">Demo Video</h3>
<p>Finally, we edited and uploaded a demo video for the Youtube channel. Check it out here:</p>
<iframe width="800" height="450" src="https://www.youtube.com/embed/PsmpY6ZeT4I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><summary type="html"><![CDATA[More sophisticated evaluation metrics]]></summary></entry><entry><title type="html">Week 4: June 19 ~ June 25</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week4/" rel="alternate" type="text/html" title="Week 4: June 19 ~ June 25" /><published>2023-06-27T19:05:00-04:00</published><updated>2023-06-27T19:05:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week4</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week4/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>The main goal of this week is to expand the model such that the agent can make turns in any direction at intersections instead of <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week3">always turning right</a>. In order to achieve this, the model needs to take a high-level command (LaneFollow, Left, Right, Straight) in addition to sensory input to guide it to either follow the lane or make turns at junctions. Thus, a new data collection routine needs to be implemented and the model architecture needs to be updated to incorporate the high-level command.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Improve model to support turns in any direction at intersections</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Retrieve the status of traffic lights and signs through the CARLA API</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand the currently available metrics in Behavior Metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Push existing code to Github</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Create demo videos</li>
</ul>

<h2 id="execution">Execution</h2>
<h3 id="high-level-commands">High-Level Commands</h3>
<p>The main challenge was two-fold: recording high-level commands during the data collection phase and then supplying these commands during inference time without the need to implement a route planner. Starting from CARLA version 0.9.13, we can access turning decisions via the traffic manager API, which eased the first problem. For the second issue, we opted for a hard-coded sequence of turning decisions at each junction during every testing episode, akin to following directions from a GPS navigator, such as “Left Right Left.”</p>

<p>During testing, at each frame, the high-level command defaults to LaneFollow unless the agent is at a junction. For every new junction the agent encounters, it reads the next turning decision from the sequence. The agent is gauranteed to arrive
at the target location following the turning instructions.</p>

<h3 id="model-architecture-update">Model Architecture Update</h3>
<p>As described <a href="/gsoc2023-Meiqi_Zhao/blog/2023/week1">previously</a>, the model takes the RGB camera image and semantic segmentation stacked as a 6-channel image and feeds it through a convolutional LSTM network. The output is flattened and concatenated with the speed measurement and then sent through three linear layers to produce the driving commands: throttle, steer, and brake. In order to incorporate the high-level commands, which are discre values, we employed an embedding layer. This layer produces embedding vectors of length 5 from the input high-level command, which are then concatenated with the flattened visual features and speed measurement before passed through the linear layers.</p>

<h3 id="traffic-light-problem">Traffic Light Problem</h3>
<p>During data collection, the expert vehicle is set to ignore traffic lights and signs. Naturally, the trained agent will also acquire the same behavior as in training data. This causes a problem
where the agent occasionally runs red lights, potentially colliding with other vehicles. The video below demonstrates one example where the agent almost collides with a truck as it attempts to make a turn against a red light. Interestingly, the agent was able to recover back to the lane after barely avoiding the collision. We also experimented with setting all traffic lights to green, but this solution proved ineffective as all vehicles tried to cross intersections simultaneously, essentially mirroring the original problem.</p>

<iframe src="https://drive.google.com/file/d/1AsqyY9NQbzt7j0Ip6IZhHgrA-TNkVrQc/preview" width="800" height="450" allow="autoplay"></iframe>

<h3 id="distance-to-the-leading-vehicle">Distance to the Leading Vehicle</h3>
<p>We trained our model using two slightly different datasets. For the first dataset, during data collection, the distance to the leading vehicle was set to 2.5 meters, meaning that the expert vehicle was instructed to maintain a distance of 2.5 meters from the vehicle in front of it. Following the training of our model on this dataset, we observed that although the agent did well at lane following, it frequently collided with the vehicle ahead. This prompted us to make an adjustment to the distance to the leading vehicle using the traffic manager, thus generating a second dataset. The evaluation results, as shown below, indicate that by making the expert agent more “cautious” during data collection, the trained agent will also try to maintain a larger distance from other vehicles, effectively reducing the risk of collision.</p>

<h3 id="evaluation-metrics">Evaluation Metrics</h3>
<p>We explored two evalaution metrics: success rate and distance traveled before a collision occurs for failed cases. All testing is done in Town02, a map unseen to the model during training.
We sampled 12 routes in Town02 for evaluation, each consisting of exactly two turns.</p>

<p>We compared the models trained on the two datasets as described above.</p>
<ul>
  <li><strong>v6.1</strong>: expert maintains a distance of 2.5 meters to leading vehicle
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>success rate: 0.417
average distance traveled before collision (failed cases): 94.04m
</code></pre></div>    </div>
  </li>
  <li><strong>v6.2</strong>:  expert maintains a distance of 4 meters to leading vehicle
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>success rate: 0.083 |
average distance traveled before collision (failed cases): 88.22m
</code></pre></div>    </div>
  </li>
</ul>

<p>For the next steps, it would also be interesting to evaluate success rate weighted by distance.</p>

<h3 id="demo">Demo</h3>
<p>The two videos below demonstrate a successful and a failed episode during testing time respectively.</p>
<iframe src="https://drive.google.com/file/d/1xqDArqP-86vZy0_beMsvcXG2nI53jffm/preview" width="800" height="450" allow="autoplay"></iframe>

<iframe src="https://drive.google.com/file/d/1d6AOEehk5IqmbZEvFPJKB25hxxPHEgpr/preview" width="800" height="450" allow="autoplay"></iframe>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="Training" /><category term="Evaluation" /><category term="ImitationLearning" /><category term="DataCollection" /><summary type="html"><![CDATA[Enhancing the agent's ability to make turning decisions]]></summary></entry><entry><title type="html">Week 3: June 12 ~ June 18</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week3/" rel="alternate" type="text/html" title="Week 3: June 12 ~ June 18" /><published>2023-06-19T04:07:00-04:00</published><updated>2023-06-19T04:07:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week3</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week3/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>On Monday, we reviewed the performance of the model that we trained in Week 2. The model surpassed our initial expectations, demonstrating a surprising ability to follow the correct lane, pause when encountering vehicles upfront, and resume driving once the path ahead was clear. However, the model also showed a tendency to occasionally stop unnecessarily or collide with other vehicles.</p>

<p>While the model passed most of our test scenarios, two significant limitations came to the forefront. 1. the model’s navigation abilities were limited to straight routes without intersections. 2. the testing and evaluation of the model were conducted exclusively in Town01, the same map where the training took place.</p>

<p>Moving forward into Week 3, we aim to overcome these limitations. Our objective for this week is to enhance the model to become a more versatile lane follower — one that can navigate through diverse routes, including those with turns. Additionally, we plan to commence the evaluation of our models in an unseen environment. By testing the model on a new map, we can further ensure its ability to generalize and adapt to novel scenarios.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Experiment with removing the irrelevant part from segmentation and RGB images</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Experiment with affine transformations in addition to noise injection</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Try routes with turns and intersections</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add more obstacles such as pedestrians</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explore which types of vehicles are present in the dataset</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Develop evaluation metrics</li>
</ul>

<h2 id="execution">Execution</h2>

<h3 id="carla-agentapi--traffic-manager">CARLA AgentAPI → Traffic Manager</h3>
<p>Previously we had been using CARLA’s Behavior Agent to generate expert demonstrations for our imitation learning algorithm. The first problem we had to deal with was that the CARLA Behavior Agent was not able to reliably make turns without running into obstacles such as pavements or street light poles. There are several issues on CARLA’s github repo page reporting various similar problems with the Behavior Agent. The general advice from the community is to use the server-side autopilot, which is coordinated by the Traffic Manager, instead of the Agent API. The Traffic Manager offers more reliable and robust driving behavior, making it a better choice for our data collection process. 
Thus our first step this week was to re-implement the data collection tool to leverage the Traffic Manager for controlling the ego-vehicle during data collection.</p>

<figure>
  <center>
  <img src="/gsoc2023-Meiqi_Zhao/assets/img/right_turn_run_into_pavement.gif" width="500" />
  <figcaption>Behavior Agent runs into pavement when making a right turn.</figcaption>
  </center> 
</figure>

<h3 id="turning-decisions">Turning Decisions</h3>
<p>Fortunately, starting from CARLA version 0.9.13, the Traffic Manager now supports setting routes and paths for individual vehicles, which makes it possible for us to conveniently set the agent to always turn right at intersections to simplify the turning decisions. In our current setup, we have strategically selected testing routes that enable the ego-vehicle to reach its target destination by always making right turns at intersections. By adopting this approach, we can focus on training our agent to learn how to make turns without the need for a complex global navigator.</p>

<h3 id="traffic-light">Traffic Light</h3>
<p>After switching to server-side autopilot by the Traffic Manager and training models on the new data, we realized that the trained agent tends to halt indefinitely in front of the traffic light regardless of its status. To address this issue, we implemented a simple workaround following codevilla et al.[1]: during data collection, we instructed the autopilot expert agent to ignore the traffic lights altogether. By doing so, we ensured that the agent would continue moving without halting at red lights.  However, it is important to note that other vehicles on the map still adhere to traffic rules, including waiting for the traffic lights to turn green before proceeding. As a result, if there is a line of vehicles ahead of our expert agent at an intersection, it will need to wait for those vehicles to start moving before it can proceed safely. This workaround allows us to collect data effectively while still considering the behavior of other vehicles on the road. Our expert agent can navigate through intersections smoothly by taking into account the actions of surrounding vehicles.</p>
<video width="800" controls="">
  <source src="/gsoc2023-Meiqi_Zhao/assets/video/v5.1_stops_indefinitely.mp4" type="video/mp4" />
</video>

<h3 id="affine-transformation">Affine Transformation</h3>
<p>Besides noise injection during the data collection process as described in previous week’s post, we experimented with applying an affine transformation to images during training. The concept behind this transformation is to shift the images slightly to the left or right, simulating real-world scenarios where the vehicle may experience lateral displacement on the road. Note that we also adjust the recorded steering angle in the opposite direction of the image shift. This ensures that the model learns to compensate for the lateral displacement and effectively navigate the road, maintaining a consistent trajectory.</p>

<center> <img src="/gsoc2023-Meiqi_Zhao/assets/img/affine transformation.png" width="500" /></center>

<h3 id="class-filtering">Class Filtering</h3>
<p>We also experiemented on masking out everything in the RGB and segmentation images except the vehicles, pedestrians, roads, and road lines. The image below depicts one example: (ignore the “Original” in the titles)</p>

<center> <img src="/gsoc2023-Meiqi_Zhao/assets/img/masked_rgb_and_segmentation.png" width="500" /></center>

<h3 id="models">Models</h3>
<p>We trained and compared three models:</p>

<ul>
  <li><strong>v5.1</strong>: follow traffic lights and signs, noise injection (as described in previous week’s post)</li>
  <li><strong>v5.2</strong>: ignore traffic lights and signs, noise injection</li>
  <li><strong>v5.3</strong>: ignore traffic lights and signs, noise injection + affine transformation + class filter</li>
</ul>

<h3 id="evaluation">Evaluation</h3>
<p>We evaluated the three models in Town02, a map unseen to the model during training, and compared their performance. There are 5 testing routes, each consisting of multiple intersections where the agent need to make a right turn.
The table belows shows the mean distance traveled before collision in failed cases and the standard deviation.</p>
<center> <img src="/gsoc2023-Meiqi_Zhao/assets/img/v5.1_v5.4_comparision.png" width="500" /></center>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Mean (m)</th>
      <th>SD (m)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>v5.1</td>
      <td>45.60</td>
      <td>50.75</td>
    </tr>
    <tr>
      <td>v5.3</td>
      <td>135.05</td>
      <td>72.16</td>
    </tr>
    <tr>
      <td>v5.4</td>
      <td>166.7</td>
      <td>123.21</td>
    </tr>
  </tbody>
</table>

<h3 id="demo">Demo</h3>

<video width="800" controls="">
  <source src="/gsoc2023-Meiqi_Zhao/assets/video/v5.4_demo.mp4" type="video/mp4" />
</video>

<h2 id="references">References</h2>
<p>[1] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, &amp; Alexey Dosovitskiy. (2018). End-to-end Driving via Conditional Imitation Learning.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="Training" /><category term="ImitationLearning" /><category term="DataAugmentation" /><summary type="html"><![CDATA[Train lane follower that can make turns]]></summary></entry><entry><title type="html">Week 2: June 5 ~ June 11</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week2/" rel="alternate" type="text/html" title="Week 2: June 5 ~ June 11" /><published>2023-06-12T04:04:00-04:00</published><updated>2023-06-12T04:04:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week2</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week2/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>During Monday’s meeting, we discussed last week’s progress on developing the data collection tool and our initial analysis of the model, which was trained on the preliminary dataset. Nikhil highlighted an issue in my approach that I was using an array of semantic labels as the 4th channel to augment the RGB image data from the camera. He suggested a more effective method would be to transform these labels into RGB values, treating them in the same manner as the camera image for better feature learning.</p>

<p>Data augmentation was another important aspect of imitation learning. Our last week’s model faced a challenge in self-correction: it would swiftly leave the lane if it deviated from the center, primarily because it was never exposed to situations where the expert demonstrates steering back from a drift. To improve this, we considered augmenting our dataset to include scenarios simulating recovery from disturbances. One potential solution involves applying an affine transformation to slightly shift the images left or right and adjust the steering values accordingly. This technique would expose the agent to scenarios where images are offset to one side and the steering angle is towards the opposite direction, thus teaching it how to auto-correct if it strays from the center of the lane.</p>

<p>An alternative strategy involves the injection of temporally-correlated noise into the expert agent’s driving control commands. For instance, noise that prompts the car to drift to the left could be incorporated into the steering commands by the agent. While this combined signal is used for vehicle control, only the expert agent’s counteractive response to the drift-induced noise is recorded as a part of the dataset.</p>

<p>Given time constraints, I could only experiment with the latter approach this week. But the good news is, it proved to be quite effective! For more details, refer to the Execution section.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Convert semantic segmentation from labels to RGB images</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Plot out predictions vs ground truth for model evaluation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Perform data augmentation by applying custom transformations (Did noise injection instead)</li>
</ul>

<h2 id="execution">Execution</h2>

<h3 id="bug-fixing">Bug Fixing</h3>
<ul>
  <li>One of the first steps taken was to convert semantic segmentation labels into RGB values, integrating the segmentation and camera image into a 6-channel model input.</li>
  <li>There were instances in the previously collected data where the agent overstepped the target location slightly before executing a 180-degree turn. To counter this, I reduced the time step of the synchronous mode in CARLA and also manually sifted through the episodes by monitoring the data collection process.</li>
  <li>There was an anomaly with the RGB images collected in the previous week. Each time step did not correctly update the images, leading to discrepancies between the synchronization of the RGB images and the semantic segmentation samples. This inconsistency stemmed from the data collector using the same reference, rather than saving a distinct copy of each camera frame.</li>
</ul>

<h3 id="noise-injection">Noise Injection</h3>
<p>The image below shows a (half) period of sine wave injected to the steer commands from the expert agent. The agent is able to correct the drift caused by the noise by steering to the opposite direction.</p>

<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/noise_injection.png" width="500" /></center>

<p>As shown in the video below, the agent drifts a little bit to the left or right from time to time due to the injected noise but it is able to steer back to the center of the lane:</p>

<video width="800" controls="">
  <source src="/gsoc2023-Meiqi_Zhao/assets/video/noisy_data_collection-2023-06-11_23.52.06.mp4" type="video/mp4" />
</video>

<h3 id="demo">Demo</h3>

<video width="800" controls="">
  <source src="/gsoc2023-Meiqi_Zhao/assets/video/v4_demo.mp4" type="video/mp4" />
</video>

<h2 id="references">References</h2>
<p>[1] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, &amp; Alexey Dosovitskiy. (2018). End-to-end Driving via Conditional Imitation Learning.</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="Training" /><category term="DataCollection" /><category term="ImitationLearning" /><category term="DataAugmentation" /><summary type="html"><![CDATA[Train a better model with noise-injected data]]></summary></entry><entry><title type="html">Week 1: May 29 ~ June 04</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week1/" rel="alternate" type="text/html" title="Week 1: May 29 ~ June 04" /><published>2023-05-29T13:17:00-04:00</published><updated>2023-05-29T13:17:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week1</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/week1/"><![CDATA[<h2 id="preliminaries">Preliminaries</h2>
<p>The start of the official coding period for GSoC is here! In Monday’s meeting, we revisited the logistics and strategized the initial steps for implementing the data collection tool. Our approach is grounded in simplicity; we begin with basic scenarios and gradually integrate more complexity. Here are a few elementary scenarios we considered:</p>
<ul>
  <li>Straight routes with no intersections or turns, as marked by red, green, and blue arrows in the image below, where other vehicles serve as dynamic obstacles.</li>
  <li>Straight routes with no intersections or turns, but with a traffic light and a pedestrian crossing.</li>
  <li>Straight routes with no intersections or turns, but with a parked car partially obstructing the lane, demanding the ego vehicle to circumvent the obstacle without changing lanes.</li>
</ul>

<p>While we would like the route to be simple in the beginning, it is crucial to introduce dynamic obstacles proximate to the ego vehicle to foster its learning of obstacle avoidance. 
In addition, we decided the implementation of the data collection tool should be an iterative process, allowing us to test the data samples at each stage with incrementally more sophisticated models.</p>

<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/Town01_simple_routes.png" width="500" /></center>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Get started with the data collection tool and collect some sample data</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Delve into the simulator to understand how to configure sensors, extract data, and spawn pedestrians crossing roads</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Check out Deep Learning Studio[1] and review existing models</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Train a small model on simplest scenarios if time permits</li>
</ul>

<h2 id="execution">Execution</h2>

<h3 id="data-collection-tool">Data Collection Tool</h3>

<p>The first step of the execution was implementing a data collection tool. This tool randomly samples straight routes with no intersections from a list of start and target locations provided in a .txt file. For each episode, the program reloads the traffic, spawns the ego-vehicle at the start location, and utilizes CARLA’s <a href="https://carla.readthedocs.io/en/0.9.12/adv_agents/">BehaviorAgent</a> to drive it to the target location. For each frame, the RGB camera image, semantic segmentation, current speed measurement, and the controls (throttle, steer, brake) are recorded and saved in .pkl format.</p>

<p>For sample data I collected 100 episodes, totalling ~32000 frames.</p>

<h3 id="model">Model</h3>
<p>Simultaneously, I explored the Deep Learning Studio[1] and reviewed several existing models. From this exploration, I modified the DeepestLSTMTinyPilotNet to accept a 4D image input (RGB+Segmentation) and concatenate the flattened features with the current speed measurement. The output of the model is a vector of (throttle, steer, brake).</p>

<p>Below is the architecture of the modified DeepestLSTMTinyPilotNet:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DeepestLSTMTinyPilotNet(
  (cn_1): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2))
  (relu_1): ReLU()
  (cn_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (relu_2): ReLU()
  (cn_3): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (relu_3): ReLU()
  (dropout_1): Dropout(p=0.2, inplace=False)
  (clstm_n): ConvLSTM(
    (cell_list): ModuleList(
      (0): ConvLSTMCell(
        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
      (1): ConvLSTMCell(
        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
      (2): ConvLSTMCell(
        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
    )
  )
  (fc_1): Linear(in_features=6721, out_features=50, bias=True)
  (relu_fc_1): ReLU()
  (fc_2): Linear(in_features=50, out_features=10, bias=True)
  (relu_fc_2): ReLU()
  (fc_3): Linear(in_features=10, out_features=3, bias=True)
)
</code></pre></div></div>
<p>The figure below shows the training and validation loss over 100 epochs.</p>

<center><img src="/gsoc2023-Meiqi_Zhao/assets/img/DeepestLSTMTinyPilotNet_100epochs_loss.png" width="500" /></center>

<h3 id="results">Results</h3>
<p>I evaluated the performance of the model by deploying it in the CARLA simulator and observing its behavior. As shown in the video below, there are several problems: (1) the vehicle is hitting brake even when there is not obstacle in front of it (2) the vehicle steers to the left instead of driving a straight trajectory</p>

<video width="800" controls="">
  <source src="/gsoc2023-Meiqi_Zhao/assets/video/DeepestLSTMTinyPilotNetwork_100eopchs.mp4" type="video/mp4" />
</video>

<h2 id="references">References</h2>
<p>[1] Deep Learning Studio: https://github.com/JdeRobot/DeepLearningStudio</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="Coding" /><category term="DataCollection" /><category term="CARLA" /><category term="AutonomousDriving" /><category term="ImitationLearning" /><summary type="html"><![CDATA[implementation of data collection tool]]></summary></entry><entry><title type="html">Community Bonding: May 22 ~ May 28</title><link href="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/community-bonding-week-2/" rel="alternate" type="text/html" title="Community Bonding: May 22 ~ May 28" /><published>2023-05-28T08:00:00-04:00</published><updated>2023-05-28T08:00:00-04:00</updated><id>http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/community%20bonding%20week%202</id><content type="html" xml:base="http://localhost:4000/gsoc2023-Meiqi_Zhao/blog/2023/community-bonding-week-2/"><![CDATA[<p>This week marked an intense dive into literature and codebases to unravel the mechanisms and best practices of data collection. In Monday’s meeting, we went over our findings from last week’s literature review on state-of-the-art imitation learning algorithms for autonomous driving and set goals for this week. The focus was to comprehend and dissect the data collection methodologies utilized in various research papers.</p>

<h2 id="objectives">Objectives</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Detailed analysis of various research papers with a focus on their data collection methods.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />(Partially Completed) Explore the corresponding codebases to gain a practical understanding of the implementation.</li>
</ul>

<h2 id="findings">Findings</h2>
<p>This week, I revisited the work done by Codevilla et al.[1], Hesham et al.[2], Chen et al.[3] and went over their codebases to understand how data collection was implemented.</p>

<h3 id="overall">Overall</h3>
<ul>
  <li><strong>Expert demonstration</strong>: Many works leverage CARLA’s built-in autopilot function to gather expert demonstrations for imitation learning. However, it is also possible to use a human driver’s input for a more realistic demonstration.</li>
  <li><strong>Simulation environment</strong>: Usually in CARLA simulator, Town01 is used for training and Town02 is reserved for testing.</li>
  <li><strong>Format</strong>: In our case, each episode should consist of a sequence of tuples of (RGB image, semantic segmentation, measurements, control commands)</li>
  <li><strong>Data variability</strong>: To improve the robustness of the learned policy, the data collected should cover various driving conditions including different weather conditions, traffic densities, and times of the day.</li>
  <li><strong>Data Augmentation</strong>: This technique enriches the training set by simulating ‘imperfect’ scenarios and applying random transformations to each image during training, enhancing the model’s ability to handle diverse situations.</li>
  <li><strong>Action Space</strong>: Depending on the work, it can include the steering angle, the acceleration, the braking amount, or even high-level commands like ‘turn left’, ‘turn right’, ‘go straight’, etc.
    <ul>
      <li><strong>Route Planning</strong>: If high-level commands are used to determine the direction of the vehicle at intersections, a navigator/planner could be used to derive the current high-level command for each frame. However, in our case, we would want to start small with routes that doesn’t contain intersections.</li>
    </ul>
  </li>
  <li><strong>Episode Duration</strong>: Each episode should be long enough to contain meaningful driving behaviour but short enough to fit into the memory. It is common to set a maximum number of frames (e.g. 5,000) per episode.</li>
</ul>

<h3 id="data-augmentation">Data Augmentation</h3>
<p>In my literature review, I found that many papers, including the two other papers listed, adopted the data augmentation practices introduced by Codevilla et al. These practices focus on two main aspects:</p>

<ul>
  <li>
    <p>Addressing the lack of ‘imperfect’ scenarios in the training data: As the training data typically lacks demonstrations of the agent recovering from non-ideal situations, it’s necessary to augment the data with such instances. This can be achieved by injecting temporally correlated noise into the expert’s driving commands during data collection. This simulates gradual deviation from the correct path and abrupt disturbances. However, only the expert driver’s actual control commands are used in the training data.</p>
  </li>
  <li>
    <p>Applying random transformations to each image during training: These transformations include adjusting the image’s contrast and brightness, adding Gaussian and salt-and-pepper noise, and introducing region dropout by masking a small section of the image black.</p>
  </li>
</ul>

<p>Note that some transformations such as tranlation or ratation should not be applied, as the control commands are not invariant of these transformations.</p>

<h3 id="data-storage">Data Storage</h3>
<p>Below are a few commonly-used data storage paradigms:</p>
<ul>
  <li><strong>Images &amp; json files</strong>: RGB and semantic segmentation images can be stored as standard image files. JSON files can hold associated metadata for each frame such as well as the control commands and measurements. This format is human-readable and easily manipulated but may be slower to read/write in large quantities.</li>
  <li><strong>Pickle</strong>: This is a Python-specific binary serialization format. Objects can be directly serialized and deserialized to and from byte streams, which makes storing complex objects convenient. However, it may not be suitable for very large datasets due to memory constraints.</li>
  <li><strong>LMDB</strong>: This is a fast, memory-efficient database library. It can be used to store large amounts of data without loading the entire database into memory. This can greatly accelerate data retrieval and is especially useful for larger datasets.</li>
</ul>

<h3 id="useful-tools">Useful Tools</h3>
<p>There are some existing tools that can be mofidied to suit our needs or used as reference:</p>
<ul>
  <li><strong>Carla Data Collector</strong>(version 0.8.4): This tool allows the user to configure a dataset configuration file that contains a set of start/target positions, sensor settings, traffic settings etc. to generate a set of eepisodes from an expert demonstrator.</li>
  <li><strong>Carla Driving Benchmark</strong>(version 0.8.4): This library provides a planner that uses a graph based approach and A* algorithm to find a path from the start location to the target location. It convieniently allows querying the high-level command at the current position.</li>
</ul>

<h3 id="next-steps">Next Steps</h3>
<p>The plan is to incrementally increase the complexity of the scenarios from which we collect data. This approach ensures the fundamentals are in place before we dive into more advanced setups. Here’s an outline:</p>

<ul>
  <li>
    <p><strong>Non-Intersection Lane Following with a Stationary Obstacle</strong>: Begin with the simplest case: collect data where the agent doesn’t encounter any intersections and only needs to follow the current lane, with one parked vehicle on its path. This will give us a feel for the data collection process, and help us understand the interaction between the agent and static objects in the environment.</p>
  </li>
  <li>
    <p><strong>Non-Intersection Lane Following with Dynamic Traffic</strong>: Once we are comfortable with the basic data collection process, add more complexity by including moving cars on the map. This will give us a deeper understanding of how to handle dynamic objects in the environment.</p>
  </li>
  <li>
    <p><strong>Right Turn Only at Intersections</strong>: With a solid understanding of data collection in simple non-intersection scenarios, we can then proceed to collect episodes where the agent only needs to turn right at intersections to arrive at the target location. This will bring into play the complexities of intersection navigation and multi-lane traffic.</p>
  </li>
  <li>
    <p><strong>Conditional Imitation Learning</strong>: Finally, we can try to collect data in a way that matches the Conditional Imitation Learning paper, i.e., also record high-level commands like TURN_LEFT, TURN_RIGHT, GO_STRAIGHT, FOLLOW, etc. This will complete our imitation learning data collection procedure and lay a solid foundation for our following training and evaluation stages.</p>
  </li>
</ul>

<h2 id="references">References</h2>
<p>[1] Codevilla, Felipe et al. “End-to-End Driving Via Conditional Imitation Learning.” 2018 IEEE International Conference on Robotics and Automation (ICRA) (2017): 1-9.</p>

<p>[2] Hesham M. Eraqi, Mohamed N. Moustafa, Jens Honer. Dynamic Conditional Imitation Learning for Autonomous Driving. IEEE Transactions on Intelligent Transportation Systems (ISSN: 1524-9050, Online ISSN: 1558-0016). Issue 12, Vol 23, Pages 22988-23001. DOI: 10.1109/TITS.2022.3214079, December 2022. [Impact Factor: 9.551]</p>

<p>[3] Chen, Dian et al. “Learning by Cheating.” Conference on Robot Learning (CoRL). 2019.</p>

<p>[4] Carla Data Collector: https://github.com/carla-simulator/data-collector/tree/master</p>

<p>[5] Carla Driving Benchmark: https://github.com/carla-simulator/driving-benchmarks/tree/master</p>]]></content><author><name></name></author><category term="WeeklyUpdates" /><category term="LiteratureResearch" /><category term="DataCollection" /><category term="CARLA" /><category term="AutonomousDriving" /><summary type="html"><![CDATA[second week of the community bonding period; research data collection methods]]></summary></entry></feed>